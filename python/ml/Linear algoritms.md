**Бутстрап** - несколько раз взять N объектов с возвращением сгенерировав несколько подвыборок.

**Бэггинг** - обучить N классфикатороров на N подвыборках, полученных бустингом. 

Наивный Байесовский классификатор
------
Считаем для каждого независимого от других класса $P(C|O)$ и складываем вероятности там где _True_ при предиктинге. 
$$P(C|O) = \frac{P(O|C)P(C)}{P(O)}$$

Линейная регрессия
--------
Зная X, мы должны найти Y, и цель линейной регрессии заключается в поиске значений коэффициентов B0 и B1 $\large y = b_0 + b_1x$

Логистическая регрессия
---------
Позволяет дать не только качественный но и количественный результат.
Как и все регрессионные модели могут быть записаны в виде формулы:
$$Y = F(x_1,x_2,...,x_n) = a + b_1x_1 + ... + b_nx_n$$
Будем предсказывать не $0/1$ а $P(y)$:
$$P(y) = \frac{1}{1+e^{-y}}$$
$$P'(y) = \log_{e}{(\frac{P}{1-P})}$$


 

Линейный дискриминантный анализ (LDA)
------
Для каждого класса ищем средние значения и среднее отклонение. Предсказания производятся путём вычисления дискриминантного значения для каждого класса и выбора класса с наибольшим значением.

K-ближайших соседей (KNN)
-------
Ищем для каждой вершины ближайших соседей и выбираем наиболее часто встретившийся класс.


Сети векторного квантования (LVQ) (Learning vector quantization)
-------
Чтобы не хранить всю обучающую выборку подберем _N_ векторов так, чтобы они проходили через "середины" наших кластеров и ищем расстояния до них. Работает как прострейшая неросеть с 1 слоем Кохонена, в котором подбираются коэффициенты $w_1..w_i$ 

Метод опорных векторов (SVM)
------
Cтроим _N_ гиперплоскостей, которые обозначали бы границы кластеров.






Деревья принятия решений
------
Есть 2 подхода к выбору параметра для разбиения:
1. __Теоретико-информационный критерий__ - тот по которому уменьшение энтропии максимально. 

$$ H = -\sum_{i=1}^n \frac{N_i}{N} log(\frac{N_i}{N})$$

* n — число классов в исходном подмножестве, N_iN 
* i — число примеров i-го класса, NN — общее число примеров в подмножестве.

2. __Статистический подход__ - тот, при котором индекс джинни минимален (разбиение максимально сооьвеьсвует разбиению ответов из примера) 

$$ Gini(Q) = 1-\sum_{i=1}^n p_i^2$$
* Q — результирующее множество, nn — число классов в нём, p_ip 
* i — вероятность i-го класса


Основные параметры модели:
* `n_estimators` — число деревьев в "лесу"
* `criterion` — критерий для разбиения выборки в вершине
* `max_features` — число признаков, по которым ищется разбиение
* `min_samples_leaf` — минимальное число объектов в листе
* `max_depth` — максимальная глубина дерева

Градиентный бустинг
----
Будем искать финальный алгоритм классификации в виде композиции:
$$\large F_M(x) = \sum_{m=1}^M b_m h(x;a_m)$$
$h(x)$ - элементарная модель, например лес деревьев

При добавлении новой модели мы уже имеем ансабль $F_{n-1}(x)$, тогда применив метод жадного наращивания:
$$\large F_n(x) = F_{n-1}(x) + b_mh(x;a_m)$$

Чтобы компесировать недостатки предыдущих моделей новая должна быть равна $\nabla Q$ от $L(Y,F_{n-1}(x))$:
$$\large F_n(x) = F_{n-1}(x) - b_m \nabla Q$$

Теперь нужно построить алгоритм, который будет выдавать значения примерно равные вектору $\nabla Q$
$$\large a_m = argmin \sum_{i=1}^N L(\nabla Q_i,h(x_i,a))$$

Построим $h(x)$. Теперь скорректируем вес нашей модели $b_m$
$$\large b_m = argmin \sum_{i=1}^N L(y_i,F_{n-1}(x_i) - b  h(x_i,a))$$

Ура! Теперь можем приступить к следующей итерации..

### Вариации бустинга:

* __Стохастический градиентный бустинг__ - на каждом шаге алгоритма новое слагаемое считается опираясь не на всю обучающую выборку, а лишь на случайную подвыборку фиксированного размера. 

* __Техника случайных подпространств__ - обучаемся на подвыборке данных и подвыборки признаков.

* __Мультиклассовая классификация__ -  Идея бустинга для бинарной классификации легко обобщается на случай K классов. Вводится следующая функция потерь:
$$\large L(y,F) = - \sum_{i=1}^k y_i * log_{p_i}(x)$$

* __AdaBoost__ -  используется экспоненциальная функция потерь
$$\large L(y,F) = exp(-yF)$$

* __LogitBoost__ -  используется логарифмическая функция потерь
$$\large L(y,F) = log(1+exp(-2yF))$$

