

### Гессиан функции $H(x)$ - определитель матрицы вторых производных (матрица Гессе)

Метод наименьших квадратов
----
По формуле $Ax=y$ ошибка $Ax-b$ а квадрат ошибки: $(Ax-b)^T(Ax-b)$
$$\large x = (A^TA)^{-1}A^Ty$$


Метод максимального правдоподобия
----
Ищем такие параметры $(x_1 .. x_n)$ при которых **функции правдоподобия** максимальна. Берем производную от $f(x)$  Приравниваем ее к 0 (находим параметр при котором функция правдоподобия максимальна). Возьмем от функции правдоподобие логарифм чтобы перейти от полинома к сумме (п.2). Минимум у логарифма и линейной функции одинаковый.

* $\Lambda_\theta(X) = \prod\limits_{i=1}^n f_\theta(x_i)$ - функция правдоподобия.

* $L(X,\theta) = \sum\limits_{i=1}^n \ln f_\theta(x_i)$ -  логарифмическая функция правдоподобия.

$$\large \begin{array}{rcl} \log p\left(\vec{y} \mid X, \vec{w}\right) &=& \log \prod_{i=1}^n \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right) \\ &=& \sum_{i=1}^n \log \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right) \\ &=& -\frac{n}{2}\log 2\pi\sigma^2 -\frac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - \vec{w}^T \vec{x}_i\right)^2 \end{array}$$

Градиентный спуск
----
Имея несколько параметров X (веса сети) и дифференцируя по ним функцию получаем вектор частичных производных или вектор градиента:
$$\large \nabla f(x) = \langle\frac{\delta f}{\delta x_1}, \frac{\delta f}{\delta x_2}, ...,\frac{\delta f}{\delta x_n}\rangle$$

Затем изменяем параметры в сторону отрицательного градиента: $x_{i+1} = x_i - \alpha \nabla f(x_i)$, где $\alpha$ — некоторый параметр скорости обучения (learning rate).


Метод Ньютона (Newton's Method)
----
Апроксимируем нашу функцию на квадратичную (у нее есть точка минимума). Разложим через ряд Тейлора:
$$\large f(x+\delta) \approx f(x)+f'(x)\delta +\frac{1}{2}\delta f''(x)\delta$$
Теперь найдем минимум этой квадратичной функции, для этого приравняем значение ее производной к 0.
$$\large \frac{d}{dx}\bigg(f(x)+f'(x)\delta+\frac{1}{2}\delta f''(x)\delta\bigg) = f'(x) + f''(x)\delta=0\implies\delta=-\frac{f'(x)}{f''(x)}$$
$$\large x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}$$
А в общем виде для многомерного пространства:
$$\large x_{n+1}=x_n-\frac{\nabla f(x_n)}{H(f)(x)}$$

Мы хотим найти минимум функции (там где $f'(x) = 0$), будем искать через метод ньютона 